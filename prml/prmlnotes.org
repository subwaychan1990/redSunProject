#+TITLE:  Pattern Recognition and Machine Learning Notes
#+OPTIONS: ::t
#+latex:\newpage

* Introduction


*** main


#+CAPTION: ch01-init
#+BEGIN_SRC python :results silent :session src:1-1
  import numpy as np
  import matplotlib.pyplot as plt
  import sys
  sys.path.append("my-packages")
  from polynomial import PolynomialFeatures
  from linear_regressor import LinearRegressor
  from ridge_regressor import RidgeRegressor
  import pandas as pd
  pd.options.display.max_rows = 10
  from tabulate import tabulate
  tbl = lambda x: tabulate(x,headers="keys",tablefmt="orgtbl")
#+END_SRC

The problem of searching for patterns in data is a fundamental one and has a long and successful history.
For instance, the extensive astronomical observations of Tycho Brahe in the 16th century allowed Johannes Kepler to discover the empirical laws of planetary motion, which in turn provided a springboard for the development of classical mechanics.
Similarly, the discovery of regularities in atomic spectra played a key role in the development and verification of quantum physics in the early twentieth century.
The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.

#+CAPTION: Examples of hand-written digits taken from US zip codes.
#+ATTR_LaTeX: scale=0.75
#+LABEL: fig:1.1
[[file:img/fig:1.1.png]]

Consider the example of recognizing handwritten digits, illustrated in Figure [[fig:1.1]]. Each digit corresponds to a $28×28$ pixel image and so can be represented by a vector x comprising 784 real numbers.
The goal is to build a machine that will take such a vector x as input and that will produce the identity of the digit $0, \cdots , 9$ as the output.
This is a nontrivial problem due to the wide variability of handwriting.
It could be tackled(解决) using handcrafted rules or heuristics(启发法) for distinguishing the digits based on the shapes of the strokes, but in practice such an approach leads to a proliferation(增值) of rules and of exceptions(异常) to the rules and so on, and invariably(不约而同地) gives poor results.

Far better results can be obtained by adopting a machine learning approach in which a large set of N digits ${x_1,\cdots,x_N}$ called a /training set/ is used to tune(调节) the parameters of an adaptive model.
The categories of the digits in the /training set(训练集)/ are known in advance, typically by inspecting them individually and hand-labeling them.
We can express the category of a digit using /target vector(目标向量)/ $t$, which represents the identity of the corresponding digit.
Suitable techniques for representing categories in terms of vectors will be discussed later.
Note that there is one such target vector $t$ for each digit image $x$.

The result of running the machine learning algorithm can be expressed as a function $y(x)$ which takes a new digit image x as input and that generates an output vector y, encoded in the same way as the target vectors.
The precise form of the function $y(x)$ is determined during the /training phase(训练阶段)/, also known as the /learning phase(学习阶段)/, on the basis of the training data.
Once the model is trained it can then determine the identity of new digit images, which are said to comprise(包含) a /test set(测试阶段)/.
The ability to categorize correctly new examples that differ from those used for training is known as /generalization(泛化)/.
In practical applications, the variability(变化性) of the input vectors will be such that the training data can comprise only a tiny fraction of all possible input vectors, and so *generalization is a central goal in pattern recognition*.

For most practical applications, the original input variables are typically /pre-processed(预处理)/ to transform them into some new space of variables where, it is hoped, the pattern recognition problem will be easier to solve.
For instance, in the digit recognition problem, the images of the digits are typically translated and scaled so that each digit is contained within a box of a fixed size.
This greatly reduces the variability within each digit class, because the location and scale of all the digits are now the same, which makes it much easier for a subsequent(随后的) pattern recognition algorithm to distinguish between the different classes.
This pre-processing stage is sometimes also called /feature extraction(特征提取)/.
Note that new test data must be pre-processed using the same steps as the training data.

Pre-processing might also be performed in order to speed up computation.
For example, if the goal is real-time face detection(检测) in a high-resolution(高分辨率) video stream, the computer must handle huge numbers of pixels(像素) per second, and presenting these directly to a complex pattern recognition algorithm may be computationally infeasible(不可行的).
Instead, the aim is to find useful features that are fast to compute, and yet that also preserve useful discriminatory(有辨识力的) information enabling faces to be distinguished from non-faces.
These features are then used as the inputs to the pattern recognition algorithm.
For instance, the average value of the image intensity(图像灰度) over a rectangular subregion can be evaluated extremely efficiently (Viola and Jones, 2004), and a set of such features can prove very effective in fast face detection.
Because the number of such features is smaller than the number of pixels, this kind of pre-processing represents a form of dimensionality reduction(维数降低).
Care must be taken during pre-processing because often information is discarded, and if this information is important to the solution of the problem then the overall accuracy of the system can suffer.

Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as /supervised learning(监督学习)/ problems.
Cases such as the digit recognition example, in which the aim is to assign each input vector to one of a finite number of discrete categories, are called /classification(分类)/ problems.
If the desired output consists of one or more continuous variables, then the task is called /regression(回归)/.
An example of a regression problem would be the prediction of the yield in a chemical manufacturing process in which the inputs consist of the concentrations(浓度) of reactants(反应物), the temperature, and the pressure.

In other pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values.
The goal in such /unsupervised learning(无监督学习)/ problems may be to discover groups of similar examples within the data, where it is called /clustering(聚类)/, or to determine the distribution of data within the input space, known as /density estimation(密度估计)/, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of /visualization(数据可视化)/.

Finally, the technique of /reinforcement learning(反馈学习)/ (Sutton and Barto, 1998) is concerned with the problem of finding suitable actions to take in a given situation in order to maximize a reward.
Here the learning algorithm is not given examples of optimal outputs, in contrast to supervised learning, but must instead discover them by a process of trial and error.
Typically there is a sequence of states and actions in which the learning algorithm is interacting(交互) with its environment.
In many cases, the current action not only affects the immediate reward but also has an impact on the reward at all subsequent time steps.
For example, by using appropriate reinforcement learning techniques a neural network can learn to play the game of backgammon(西洋双陆棋) to a high standard (Tesauro, 1994).
Here the network must learn to take a board position as input, along with the result of a dice throw, and produce a strong move as the output.
This is done by having the network play against a copy of itself for perhaps a million games.
A major challenge is that a game of backgammon can involve dozens of moves, and yet it is only at the end of the game that the reward, in the form of victory, is achieved.
The reward must then be attributed appropriately to all of the moves that led to it, even though some moves will have been good ones and others less so.
This is an example of a /credit assignment(信用分配)/ problem.
A general feature of reinforcement learning is the trade-off(权衡) between /exploration(探索)/, in which the system tries out new kinds of actions to see how effective they are, and /exploitation(利用)/, in which the system makes use of actions that are known to yield a high reward.
Too strong a focus on either exploration or exploitation will yield poor results.
Reinforcement learning continues to be an active area of machine learning research.
However, a detailed treatment lies beyond the scope of this book.

Although each of these tasks needs its own tools and techniques, many of the key ideas that underpin(从下面支撑) them are common to all such problems.
One of the main goals of this chapter is to introduce, in a relatively informal way, several of the most important of these concepts and to illustrate them using simple examples.
Later in the book we shall see these same ideas re-emerge in the context of more sophisticated models that are applicable to real-world pattern recognition applications.
This chapter also provides a self-contained introduction to three important tools that will be used throughout the book, namely *probability theory*, *decision theory*, and *information theory*.
Although these might sound like daunting(令人生畏的) topics, they are in fact straightforward, and a clear understanding of them is essential if machine learning techniques are to be used to best effect in practical applications.


** Example: Polynomial Curve Fitting


We begin by introducing a simple regression problem, which we shall use as a running example throughout this chapter to motivate a number of key concepts.
Suppose we observe a real-valued input variable $x$ and we wish to use this observation to predict the value of a real-valued target variable $t$.
For the present purposes, it is instructive(有启发性的) to consider an artificial example using synthetically(合成地,人造地) generated data because we then know the precise process that generated the data for comparison against any learned model.
The data for this example is generated from the function $sin(2πx)$ with random noise included in the target values, as described in detail in [[Appendix A][Appendix A]].

Now suppose that we are given a training set comprising $N$ observations of $x$, written $x \equiv (x_1, \cdots, x_N)^T$ , together with corresponding observations of the values of $t$, denoted $t \equiv (t_1, \cdots , t_N )^T$. 
Figure [[fig:1.2]] shows a plot of a training set comprising $N = 10$ data points.
The input data set x in Figure [[fig:1.2]] was generated by choosing values of $x_n$, for $n = 1, \cdots, N$, spaced uniformly in range $[0,1]$, and the target data set $t$ was obtained by first computing the corresponding values of the function $sin(2πx)$ and then adding a small level of random noise having a Gaussian distribution (the Gaussian distribution is discussed in Section 1.2.4(?)) to each such point in order to obtain the corresponding value $t_n$.
By generating data in this way, we are capturing a property of many real data sets, namely that they possess an underlying regularity, which we wish to learn, but that individual observations are corrupted by random noise.
This noise might arise from intrinsically stochastic (i.e. random) processes such as radioactive decay but more typically is due to there being sources of variability that are themselves unobserved.

Our goal is to exploit this training set in order to make predictions of the value $\hat{t}$ of the target variable for some new value $\hat{x}$ of the input variable.
As we shall see later, this involves implicitly trying to discover the underlying function $sin(2πx)$.
This is intrinsically(本质地) a difficult problem as we have to generalize from a finite data set.
Furthermore the observed data are corrupted with noise, and so for a given $x$ there is uncertainty as to the appropriate value for $t$.
Probability theory, discussed in Section 1.2(?), provides a framework for expressing such uncertainty in a precise and quantitative manner, and decision theory, discussed in Section 1.5(?), allows us to exploit this probabilistic representation in order to make predictions that are optimal according to appropriate criteria.


#+CAPTION: generate data
#+BEGIN_SRC python :results silent :session src:1-1
  import pandas as pd

  def create_toy_data(func, sample_size, std):
      x = np.linspace(0, 1, sample_size)
      t = func(x) + np.random.normal(scale=std, size=x.shape)
      return x, t


  def func(x):
      return np.sin(2 * np.pi * x)


  np.random.seed(1234)
  #x_train, t_train = create_toy_data(func, 10, 0.25)
  x_obj = np.linspace(0, 1, 100)
  x_test, t_test = create_toy_data(func, 100, 0.25)
  std = 0.25
  data_train = pd.DataFrame()
  data_train["x"] = np.linspace(0,1,10)
  data_train["value"] = data_train["x"].map(func)
  data_train["t"] = data_train["value"] + \
      np.random.normal(scale=std, size=data_train["x"].shape)
  data_curve = pd.DataFrame()
  data_curve["x"] = np.linspace(0,1,100)
  data_curve["value"] = data_curve["x"].map(func)
  data_test = pd.DataFrame()
  data_test["x"] = np.linspace(0,1,100)
  data_test["value"] = data_test["x"].map(func)
  data_test["t"] = data_test["value"] + \
      np.random.normal(scale=std, size=data_test["x"].shape)
#+END_SRC


#+CAPTION: fig:1.2
#+BEGIN_SRC python :exports both :results output :session src:1-1
  plt.scatter(
      data_train["x"],
      data_train["t"],
      facecolor="none",
      edgecolor="b",
      s=50,
      label="training data")
  plt.plot(data_curve["x"], data_curve["value"], c="g",
           label="$\sin(2\pi x)$")
  plt.legend()
  plt.savefig("img/fig:1.2.png")
  plt.close("all")
#+END_SRC

#+RESULTS:

#+CAPTION: Plot of a training data set of $N = 10$ points, shown as blue circles, each comprising an observation of the input variable x along with the corresponding target variable t. The green curve shows the function $sin(2πx)$ used to generate the data. Our goal is to predict the value of $t$ for some new value of $x$, without knowledge of the green curve.
#+ATTR_LaTeX: scale=0.75
#+LABEL: fig:1.2
[[file:img/fig:1.2.png]]


For the moment, however, we shall proceed rather informally and consider a simple approach based on curve fitting.
In particular , we shall fit the data using a polynomial function of the form

\begin{equation}\label{polynomial curve fitting}
y(x, \mathbf{w}) = w_0 + w_1x+ w_2x^2 + \cdots + w_Mx^M = \displaystyle\sum_{j=0}^{M}w_jx^j
\end{equation}

where $M$ is the /order(阶数)/ of the polynomial, and $x_j$ denotes $x$ raised to the power of $j$.
The polynomial coefficients $w_0,\cdots,w_M$ are collectively denoted by the vector $\mathbf{w}$.
Note that, although the polynomial function $y(x, \mathbf{w})$ is a nonlinear function of x, it is a linear function of the coefficients $\mathbf{w}$.
Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called linear models and will be discussed extensively in Chapters 3(?) and 4(?).

The values of the coefficients will be determined by fitting the polynomial to the training data.
This can be done by minimizing an /error function(误差函数)/ that measures the misfit between the function $y(x, \mathbf{w})$, for any given value of $\mathbf{w}$, and the training set data points.
One simple choice of error function, which is widely used, is given by the sum of the squares of the errors between the predictions $y(x_n, \mathbf{w})$ for each data point $x_n$ and the corresponding target values $t_n$, so that we minimize

\begin{equation}\label{error function}
E(\mathbf{w}) = \frac{1}{2}\displaystyle\sum_{n=1}^{N}\{y(x_n, \mathbf{w}) - t_n\}^2
\end{equation}

where the factor of $1/2$ is included for later convenience.
We shall discuss the motivation for this choice of error function later in this chapter.
For the moment we simply note that it is a nonnegative quantity that would be zero if, and only if, the function $y(x, \mathbf{w})$ were to pass exactly through each training data point.
The geometrical interpretation(解释) of the sum-of-squares error function is illustrated in Figure [[fig:1.3]].

#+CAPTION: The error function \eqref{error function} corresponds to (Mone half of) the sum of the squares of the displacements (shown by the vertical green bars) of each data point from the function $y(x, \mathbf{w})$.
#+ATTR_LaTeX: scale=0.75
#+LABEL: fig:1.3
[[file:img/fig:1.3.png]]

We can solve the curve fitting problem by choosing the value of $w$ for which $E(w)$ is as small as possible.
Because the error function is a quadratic function of the coefficients $w$, its derivatives with respect to the coefficients will be linear in the elements of $w$, and so the minimization of the error function has a unique solution, denoted by $\mathbf{w^*}$, which can be found in closed form.
The resulting polynomial is given by the function $y(x, \mathbf{w^*})$.

There remains the problem of choosing the order $M$ of the polynomial, and as we shall see this will turn out to be an example of an important concept called /model comparison(模型对比)/ or /model selection(选择)/.
In Figure [[fig:1.4]], we show four examples of the results of fitting polynomials having orders $M = 0, 1, 3, 9$ to the data set shown in Figure [[fig:1.2]].


#+CAPTION: fig:1.4
#+BEGIN_SRC python :exports none :results output :session src:1-1
  for i, degree in enumerate([0, 1, 3, 9]):
      plt.subplot(2, 2, i + 1)
      feature = PolynomialFeatures(degree)
      X_train = feature.transform(data_train["x"].values)
      X_curve = feature.transform(data_curve["x"].values)
      model_train = LinearRegressor()
      model_train.fit(X_train, data_train["t"].values)
      data_train["y"] = model_train.predict(X_train)
      data_curve["y"] = model_train.predict(X_curve)

      plt.scatter(
          data_train["x"],
          data_train["t"],
          facecolor="none",
          edgecolor="b",
          s=50,
          label="training data")
      plt.plot(
          data_curve["x"], data_curve["value"], c="g",
          label="$\sin(2\pi x)$")
      plt.plot(data_curve["x"], data_curve["y"], c="r",
               label="fitting")
      plt.ylim(-1.5, 1.5)
      plt.annotate("M={}".format(degree), xy=(0.75, 1))
  plt.subplots_adjust(right=0.75)
  plt.legend(bbox_to_anchor=(1.05, 0.64), loc=2,
             borderaxespad=0.)
  plt.savefig("img/fig:1.4.png")
  plt.close("all")
#+END_SRC

#+RESULTS:

#+CAPTION: Plots of polynomials having various orders M, shown as red curves, fitted to the data set shown in Figure 1.2.
#+ATTR_LaTeX: scale=0.75
#+LABEL: fig:1.4
[[file:img/fig:1.4.png]]


We notice that the constant $(M = 0)$ and first order $(M = 1)$ polynomials give rather poor fits to the data and consequently rather poor representations of the function $sin(2πx)$.
The third order $(M = 3)$ polynomial seems to give the best fit to the function $sin(2πx)$ of the examples shown in Figure [[fig:1.4]].
When we go to a much higher order polynomial $(M = 9)$, we obtain an excellent fit to the training data.
In fact, the polynomial passes exactly through each data point and $E(\mathbf{w^*}) = 0$.
However, the fitted curve oscillates wildly and gives a very poor representation of the function $sin(2πx)$.
This latter behavior is known as /over-fitting(过拟合)/.

As we have noted earlier, the goal is to achieve good generalization by making accurate predictions for new data.
We can obtain some quantitative insight into the dependence of the generalization performance on $M$ by considering a separate test set comprising 100 data points generated using exactly the same procedure used to generate the training set points but with new choices for the random noise values included in the target values.
For each choice of $M$ , we can then evaluate the residual value of $E(\mathbf{w^*})$ given by \eqref{error function} for the training data, and we can also evaluate $E(\mathbf{w^*})$ for the test data set.
It is sometimes more convenient to use the root-mean-square (RMS) error defined by

\begin{equation}\label{RMS}
E_{RMS} = \sqrt{2E(\mathbf{w^*})/N}
\end{equation}

in which the division by N allows us to compare different sizes of data sets on an equal footing(基础), and the square root ensures that ERMS is measured on the same scale (and in the same units) as the target variable $t$.
Graphs of the training and test set RMS errors are shown, for various values of M, in Figure [[fig:1.5]].
The test set error is a measure of how well we are doing in predicting the values of $t$ for new data observations of $x$.
We note from Figure [[fig:1.5]] that small values of $M$ give relatively large values of the test set error, and this can be attributed(归结于) to the fact that the corresponding *polynomials are rather inflexible and are incapable of capturing the oscillations(震荡) in the function* $sin(2πx)$.
Values of $M$ in the range $3 \leqslant M \leqslant 8$ give small values for the test set error, and these also give reasonable representations of the generating function $sin(2πx)$, as can be seen, for the case of $M = 3$, from Figure [[fig:1.4]].


#+CAPTION: RMS
#+BEGIN_SRC python :results silent :session src:1-1
  def rmse(a, b):
      return np.sqrt(np.mean(np.square(a - b)))
#+END_SRC


#+CAPTION: fig:1.5
#+BEGIN_SRC python :exports both :results output :session src:1-1
  training_errors = []
  test_errors = []
  for degree in range(10):
      feature = PolynomialFeatures(degree)
      X_train = feature.transform(data_train["x"].values)
      X_test = feature.transform(data_test["x"].values)
      model_train = LinearRegressor()
      model_train.fit(X_train, data_train["t"].values)
      data_train["y"] = model_train.predict(X_train)
      data_test["y"] = model_train.predict(X_test)
      training_errors.append(rmse(data_train["y"],
                                  data_train["t"]))
      test_errors.append((rmse(data_test["y"],
                               data_test["t"])))
  plt.plot(
      training_errors, 'o-', mfc="none", mec="b", ms=10,
      c="b", label="Training")
  plt.plot(test_errors, 'o-', mfc="none", mec="r", ms=10,
           c="r", label="Test")
  plt.legend()
  plt.xlabel("$M$")
  plt.ylabel("$E_{RMS}$")
  plt.ylim(0,1)
  plt.savefig("img/fig:1.5.png")
  plt.close("all")
#+END_SRC

#+RESULTS:

#+CAPTION: Graphs of the root-mean-square error, defined by \eqref{RMS}, evaluated on the training set and on an independent test set for various values of $M$.
#+ATTR_LaTeX: scale=0.75
#+LABEL: fig:1.5
[[file:img/fig:1.5.png]]


#+CAPTION: tbl:1.1
#+BEGIN_SRC python :exports both :results output :session src:1-1
  mapping = {}
  for degree in [0, 1, 3, 9]:
      feature = PolynomialFeatures(degree)
      X_train = feature.transform(data_train["x"])
      model_train = LinearRegressor()
      model_train.fit(X_train,data_train["t"].values)
      mapping["$M=%d$" % degree] = pd.Series(model_train.w)
  df = pd.DataFrame(mapping)
  df.index = ["$w_%d^*$" % degree for degree in range(10)]
  print(tbl(df.round(2).fillna("")))
#+END_SRC

#+RESULTS:

#+CAPTION: Table of the coefficients $\mathbf{w^*}$ for polynomials of various order. Observe how the typical magnitude of the coefficients increases dramatically as the order of the polynomial increases.
#+LABEL: tbl:1.1
|         | $M=0$   | $M=1$   | $M=3$   |      $M=9$ |
|---------+---------+---------+---------+------------|
| $w_0^*$ | 0.05    | 0.84    | 0.04    |       0.07 |
| $w_1^*$ |         | -1.58   | 10.68   |      72.23 |
| $w_2^*$ |         |         | -31.26  |   -1553.7  |
| $w_3^*$ |         |         | 20.41   |   14489.7  |
| $w_4^*$ |         |         |         |  -71214.4  |
| $w_5^*$ |         |         |         |  202166    |
| $w_6^*$ |         |         |         | -343061    |
| $w_7^*$ |         |         |         |  342874    |
| $w_8^*$ |         |         |         | -185879    |
| $w_9^*$ |         |         |         |   42105.2  |


For $M = 9$, the training set error goes to zero, as we might expect because this polynomial contains 10 degrees of freedom corresponding to the 10 coefficients $w_0,\cdots,w_9$, and so can be tuned exactly to the 10 data points in the training set.
However, the test set error has become very large and, as we saw in Figure [[fig:1.4]], the corresponding function $y(x, \mathbf{w^⋆}) exhibits wild oscillations.

This may seem paradoxical because a polynomial of given order contains all lower order polynomials as special cases.
The $M = 9$ polynomial is therefore capable of generating results at least as good as the $M = 3$ polynomial.
($M = 9$ 的多项式因此能够产生至少与 $M = 3$ 一样好的结果。)
Furthermore, we might suppose that the best predictor of new data would be the function sin(2πx) from which the data was generated (and we shall see later that this is indeed the case).
We know that a power series expansion of the function $sin(2πx)$ contains terms of all orders, so we might expect that results should improve monotonically as we increase $M$.

We can gain some insight into the problem by examining the values of the coefficients $\mathbf{w^*}$ obtained from polynomials of various order, as shown in Table [[tbl:1.1]].
We see that, as $M$ increases, the magnitude of the coefficients typically gets larger.
In particular for the $M = 9$ polynomial, the coefficients have become finely tuned to the data by developing large positive and negative values so that the corresponding polynomial function matches each of the data points exactly, but between data points (particularly near the ends of the range) the function exhibits the large oscillations observed in Figure [[fig:1.4]].
Intuitively(直觉地), what is happening is that the more flexible polynomials with larger values of $M$ are becoming increasingly tuned to the random noise on the target values.
It is also interesting to examine the behavior of a given model as the size of the data set is varied, as shown in Figure [[fig:1.6]].
We see that, for a given model complexity, the over-fitting problem become less severe(严厉的) as the size of the data set increases.
Another way to say this is that the larger the data set, the more complex (in other words more flexible) the model that we can afford to fit to the data.
One rough(粗略的) heuristic that is sometimes advocated is that the number of data points should be no less than some multiple (say 5 or 10) of the number of adaptive parameters in the model.
However, as we shall see in Chapter 3, the number of parameters is not necessarily the most appropriate measure of model complexity.


#+CAPTION: fig:1.6
#+BEGIN_SRC python :results output :session src:1-1
  for i, sample_size in enumerate([15, 100]):
      plt.subplot(1, 2, i + 1)
      feature = PolynomialFeatures(9)
      x_train_tmp, t_train_tmp = create_toy_data(func, sample_size, 0.25)
      X_train_tmp = feature.transform(x_train_tmp)
      model = LinearRegressor()
      model.fit(X_train_tmp, t_train_tmp)
      X_curve = feature.transform(data_curve["x"])
      y = model.predict(X_curve)
      plt.scatter(
          x_train_tmp,
          t_train_tmp,
          facecolor="none",
          edgecolor="b",
          s=50,
          label="training data")
      plt.plot(
          data_curve["x"], data_curve["value"],
          c="g", label="$\sin(2\pi x)$")
      plt.plot(data_curve["x"], y, c="r", label="fitting")
      plt.ylim(-1.5, 1.5)
      plt.annotate("N={}".format(sample_size), xy=(0.75, 1))
  plt.savefig("img/fig:1.6.png")
  plt.close("all")
#+END_SRC

#+RESULTS:



#+CAPTION: Plots of the solutions obtained by minimizing the sum-of-squares error function using the $M = 9$ polynomial for $N = 15$ data points (left plot) and $N = 100$ data points (right plot). We see that increasing the size of the data set reduces the over-fitting problem.
#+LABEL: fig:1.6
[[file:img/fig:1.6.png]]


Also, there is something rather unsatisfying about having to limit the number of parameters in a model according to the size of the available training set.
It would seem more reasonable to choose the complexity of the model according to the complexity of the problem being solved.
We shall see that the least squares approach to finding the model parameters represents a specific case of /maximum likelihood(最大似然)/ (discussed in Section 1.2.5(?)), and that the over-fitting problem can be understood as a general property of maximum likelihood.
By adopting a /Bayesian/ approach, the over-fitting problem can be avoided.
We shall see that there is no difficulty from a Bayesian perspective in employing models for which the number of parameters greatly exceeds the number of data points.
Indeed, in a Bayesian model the /effective(有效)/ number of parameters adapts automatically to the size of the data set.

For the moment, however, it is instructive to continue with the current approach and to consider how in practice we can apply it to data sets of limited size where wemay wish to use relatively complex and flexible models.
One technique that is often used to control the over-fitting phenomenon in such cases is that of /regularization(正则化)/, which involves(包含) adding a penalty term to the error function \eqref{error function} in order to discourage the coefficients from reaching large values.
The simplest such penalty term takes the form of a sum of squares of all of the coefficients, leading to a modified error function of the form

\begin{equation}\label{regularized error function}
\tilde{E}(\mathbf{w}) = \frac{1}{2}\sum_{n=1}^N \{y(x_n,\mathbf{w}-t_n)\}^2 + \frac{\lambda}{2}\parallel\mathbf{w}\parallel^2
\end{equation}

the coefficient \lambda governs the relative importance of the regularization term compared with the sum-of-squares error term.
Note that often the coefficient $w_0$ is omitted(省略) from the regularizer because its inclusion causes the results to depend on the choice of origin for the target variable (Hastie et al., 2001), or it may be included but with its own regularization coefficient (we shall discuss this topic in more detail in Section 5.5.1(?)).
Again, the error function in \eqref{regularized error function} can be minimized exactly in closed form.
Techniques such as this are known in the statistics literature as /shrinkage(收缩)/ methods because they reduce the value of the coefficients.
The particular case of a quadratic regularizer is called /ridge regression(山脊回归)/ (Hoerl and Kennard, 1970).
In the context of neural networks, this approach is known as /weight decay(权值衰减)/.

#+CAPTION: fig:1.7
#+BEGIN_SRC python :exports both :results output :session src:1-1
  for i, lamb in enumerate([-18, 0]):
      plt.subplot(1, 2, i + 1)
      feature = PolynomialFeatures(9)
      X_train = feature.transform(data_train["x"])
      X_curve = feature.transform(data_curve["x"])
      model = RidgeRegressor(alpha=np.exp(lamb))
      model.fit(X_train, data_train["t"].values)
      y_curve = model.predict(X_curve)
      plt.scatter(
          data_train["x"],
          data_train["t"],
          facecolor="none",
          edgecolor="b",
          s=50,
          label="training data")
      plt.plot(data_curve["x"], data_curve["value"],
               c="g", label="$\sin(2\pi x)$")
      plt.plot(data_curve["x"], y_curve, c="r",
               label="fitting")
      plt.ylim(-1.5, 1.5)
      plt.annotate("$\ln\lambda = %d$" % lamb, xy=(0.6, 1))
      plt.annotate("M=9", xy=(-0.15, 1))
  plt.savefig("img/fig:1.7.png")
  plt.close("all")
#+END_SRC

#+RESULTS:

#+CAPTION: Plots of $M = 9$ polynomials fitted to the data set shown in Figure 1.2 using the regularized error function \eqref{regularized error function} for two values of the regularization parameter \lambda corresponding to $\ln\lambda = −18$ and $\ln\lambda = 0$. The case of no regularizer, i.e., $\lambda  = 0$, corresponding to $\ln\lambda = -\infty$, is shown at the bottom right of Figure 1.4.
#+ATTR_LaTeX: scale=0.75
#+LABEL: fig:1.7
[[file:img/fig:1.7.png]]


Figure [[fig:1.7]] shows the results of fitting the polynomial of order $M = 9$ to the same data set as before but now using the regularized error function given by \eqref{regularized error function}.
We see that, for a value of $\ln\lambda = −18$, the over-fitting has been suppressed(镇压) and we now obtain a much closer representation of the underlying function $sin(2πx)$.
If, however, we use too large a value for λ then we again obtain a poor fit, as shown in Figure [[fig:1.7]] for $\ln\lambda = 0$.
The corresponding coefficients from the fitted polynomials are given in Table [[tbl:1.2]], showing that regularization has the desired effect of reducing the magnitude of the coefficients.


#+CAPTION: tbl:1.2
#+BEGIN_SRC python :exports both :results output :session src:1-1
  import pandas as pd
  mapping = {}
  infty = float("inf")
  for index ,_ in enumerate([-infty, -18, 0]):
      feature = PolynomialFeatures(9)
      X_train = feature.transform(data_train["x"])
      alpha = np.exp(_)
      model_train = RidgeRegressor(alpha)
      model_train.fit(X_train,data_train["t"].values)
      mapping["$\ln\lambda=%f$" % _] = model_train.w
  df = pd.DataFrame(mapping)
  df.index = ["$w_%d^*$" % _ for _ in range(10)]
  from tabulate import tabulate
  tbl = lambda x: tabulate(x,headers="keys",tablefmt="orgtbl")
  print(tbl(df.round(2).fillna("")))
#+END_SRC

#+RESULTS:

#+CAPTION: Table of the coefficients $\mathbf{w^*}$ for $M = 9$ polynomials with various values for the regularization parameter \lambda. Note that $\ln\lambda = -\infty$ corresponds to a model with no regularization, i.e., to the graph at the bottom right in Figure 1.4. We see that, as the value of \lambda increases, the typical magnitude of the coefficients gets smaller.
#+LABEL: tbl:1.2
|         |   $\ln\lambda=-18$ |   $\ln\lambda=-\infty$ |   $\ln\lambda=0$ |
|---------+--------------------------+--------------------+------------------------|
| $w_0^*$ |                     0.07 |               0.07 |                   0.42 |
| $w_1^*$ |                     9.48 |              72.28 |                  -0.36 |
| $w_2^*$ |                   -25.92 |           -1554.91 |                  -0.42 |
| $w_3^*$ |                     2.61 |           14500.6  |                  -0.3  |
| $w_4^*$ |                    88.07 |          -71265.4  |                  -0.17 |
| $w_5^*$ |                  -144.36 |          202305    |                  -0.07 |
| $w_6^*$ |                   -38.89 |         -343288    |                   0.02 |
| $w_7^*$ |                   137.65 |          343094    |                   0.08 |
| $w_8^*$ |                    88.87 |         -185995    |                   0.12 |
| $w_9^*$ |                  -117.89 |           42130.9  |                   0.15 |


The impact of the regularization term on the generalization error can be seen by plotting the value of the RMS error \eqref{RMS} for both training and test sets against $\ln\lambda$, as shown in Figure [[fig:1.8]].
We see that in effect \lambda now controls the effective complexity of the model and hence determines the degree of over-fitting.

#+BEGIN_SRC python :exports both :results output :session src:1-1
training_errors
#+END_SRC

#+RESULTS:
: [0.7398432165734342, 0.5423420144782466, 0.5393290578963472, 0.22064507202789818, 0.2172572725796397, 0.15272460043762026, 0.09009886329262623, 0.06916993404147029, 0.056559837916173514, 6.077691470297929e-11]

#+BEGIN_SRC python :exports both :results output :session src:1-1
len(test_errors)
#+END_SRC

#+RESULTS:
: 100

#+CAPTION: fig:1,8
#+BEGIN_SRC python :exports both :results output :session src:1-1
  training_errors = []
  test_errors = []
  for lamb in np.linspace(-40,20,100):
      feature = PolynomialFeatures(9)
      X_train = feature.transform(data_train["x"])
      X_test = feature.transform(data_test["x"])
      model = RidgeRegressor(alpha=np.exp(lamb))
      model.fit(X_train, data_train["t"].values)
      y_train = model.predict(X_train)
      y_test = model.predict(X_test)
      training_errors.append(rmse(y_train, data_train["t"]))
      test_errors.append(rmse(y_test, data_test["t"]))

  plt.plot(np.linspace(-40,-20,100), training_errors, '-',
           mfc="none", mec="b", ms=10,
           c="b", label="Training")
  plt.plot(np.linspace(-40,-20,100), test_errors, '-',
           mfc="none", mec="r", ms=10,
           c="r", label="Test")
  plt.legend()
  plt.xlabel("$\ln\lambda$")
  plt.ylabel("$E_{RMS}$")
  plt.ylim(0,1)
  plt.savefig("img/fig:1.8.png")
  plt.close("all")
#+END_SRC

#+RESULTS:

#+CAPTION: caption
#+ATTR_LaTeX: scale=0.75
#+LABEL: fig:1.8
[[file:img/fig:1.8.png]]


The issue of model complexity is an important one and will be discussed at length in Section 1.3(?).
Here we simply note that, if we were trying to solve a practical application using this approach of minimizing an error function, we would have to find a way to determine a suitable value for the model complexity.
The results above suggest a simple way of achieving this, namely by taking the available data and partitioning it into a training set, used to determine the coefficients w, and a separate /validation set(验证集)/, also called a /hold-out set(拿出集)/, used to optimize the model complexity (either $M$ or \lambda).
In many cases, however, this will prove to be too wasteful of valuable training data, and we have to seek more sophisticated approaches.

So far our discussion of polynomial curve fitting has appealed largely to intuition.
We now seek a more principled approach to solving problems in pattern recognition by turning to a discussion of probability theory.
As well as providing the foundation for nearly all of the subsequent developments in this book, it will also give us some important insights into the concepts we have introduced in the context of polynomial curve fitting and will allow us to extend these to more complex situations.

** packages

定义多项式特征 ~class PolynomialFeatures~ .
其初始输入为维度,如$n=3$:M

#+BEGIN_SRC python :exports both :exports both :results output :session src:1-1
from polynomial import PolynomialFeatures
feature = PolynomialFeatures(3)
#+END_SRC

#+RESULTS:

对于每个行向量$a$, 有 ~transform~ 方法使得其输出为\(\{a_1^{i_1}a_2^{i_2}\cdots a_k^{i_M}\big|0\leqslant\sum_{j=1}^Mi_j\leqslant M\}\).
如果 $a$ 是一维向量, 强行将其转化为列向量.

#+BEGIN_SRC python :exports both :results output :session src:1-1
  print(feature.transform(np.array([[2, 7], [5, 3]])))
#+END_SRC

#+RESULTS:
: [[  1.   2.   7.   4.  14.  49.   8.  28.  98. 343.]
:  [  1.   5.   3.  25.  15.   9. 125.  75.  45.  27.]]

回归类

线性回归类

#+BEGIN_SRC python :exports both :exports both :results output :session src:1-1
from linear_regressor import LinearRegressor
#+END_SRC

#+RESULTS:



* Appendix A
<<Appendix A>>
